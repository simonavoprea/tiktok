# -*- coding: utf-8 -*-
"""tiktok.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rlqAYk59BrQuxph7NTRf_gTdD947pATh
"""

import pandas as pd

# Load the Excel file
file_path = '/content/drive/MyDrive/savedrecs_tiktok.xlsx'
excel_data = pd.ExcelFile(file_path)
df = excel_data.parse('savedrecs')
df.columns

# Citation Analysis: Analyze citation trends over time
citation_trends = df.groupby('Publication Year')['Times Cited, All Databases'].sum()

# Keyword Analysis: Extract and analyze the frequency of keywords
from collections import Counter

keywords_series = df['Author Keywords'].dropna().str.split(';')
keywords_flat = [keyword.strip().lower() for sublist in keywords_series for keyword in sublist]
keywords_counter = Counter(keywords_flat)
most_common_keywords = keywords_counter.most_common(10)

# Affiliation Analysis: Count the number of publications per affiliation
affiliation_series = df['Affiliations'].dropna().str.split(';')
affiliations_flat = [affiliation.strip() for sublist in affiliation_series for affiliation in sublist]
affiliation_counter = Counter(affiliations_flat)
top_affiliations = affiliation_counter.most_common(10)

# Publication Trends: Analyze the distribution of publications by year
publication_trends = df['Publication Year'].value_counts().sort_index()

import matplotlib.pyplot as plt

# Plotting Citation Trends by Year
plt.figure(figsize=(10, 6))
plt.plot(citation_trends.index, citation_trends.values, marker='o')
plt.title('Total Citations by Year')
plt.xlabel('Publication Year')
plt.ylabel('Total Citations (All Databases)')
plt.grid(True)
plt.show()

# Plotting Most Common Keywords
keywords_df = pd.DataFrame(most_common_keywords, columns=['Keyword', 'Frequency'])
plt.figure(figsize=(10, 6))
plt.barh(keywords_df['Keyword'], keywords_df['Frequency'], color='skyblue')
plt.xlabel('Frequency')
plt.title('Top 10 Most Common Keywords')
plt.gca().invert_yaxis()
plt.grid(True)
plt.show()

# Plotting Top 10 Affiliations
affiliations_df = pd.DataFrame(top_affiliations, columns=['Affiliation', 'Publication Count'])
plt.figure(figsize=(10, 6))
plt.barh(affiliations_df['Affiliation'], affiliations_df['Publication Count'], color='lightgreen')
plt.xlabel('Publication Count')
plt.title('Top 10 Affiliations by Publication Count')
plt.gca().invert_yaxis()
plt.grid(True)
plt.show()

# Plotting Publication Trends by Year
plt.figure(figsize=(10, 6))
plt.bar(publication_trends.index, publication_trends.values, color='lightcoral')
plt.title('Number of Publications by Year')
plt.xlabel('Publication Year')
plt.ylabel('Number of Publications')
plt.grid(True)
plt.show()

import seaborn as sns

# Advanced Plotting for Citation Trends by Year
plt.figure(figsize=(12, 6))
sns.lineplot(x=citation_trends.index, y=citation_trends.values, marker='o', color='blue', linewidth=2.5)
plt.fill_between(citation_trends.index, citation_trends.values, alpha=0.1, color='blue')
plt.title('Total Citations by Year with Trend', fontsize=16)
plt.xlabel('Publication Year', fontsize=14)
plt.ylabel('Total Citations (All Databases)', fontsize=14)
plt.grid(True)
plt.show()

# Advanced Plotting for Most Common Keywords
plt.figure(figsize=(12, 6))
sns.barplot(x='Frequency', y='Keyword', data=keywords_df, palette='viridis')
plt.title('Top 10 Most Common Keywords', fontsize=16)
plt.xlabel('Frequency', fontsize=14)
plt.ylabel('Keyword', fontsize=14)
plt.grid(True, axis='x')
plt.show()

# Advanced Plotting for Top 10 Affiliations
plt.figure(figsize=(12, 6))
sns.barplot(x='Publication Count', y='Affiliation', data=affiliations_df, palette='magma')
plt.title('Top 10 Affiliations by Publication Count', fontsize=16)
plt.xlabel('Publication Count', fontsize=16)
plt.ylabel('Affiliation', fontsize=16)
plt.xticks(fontsize=16)
plt.yticks(fontsize=16)
plt.grid(True, axis='x')
plt.show()

# Advanced Plotting for Publication Trends by Year
plt.figure(figsize=(12, 6))
sns.barplot(x=publication_trends.index, y=publication_trends.values, palette='coolwarm')
plt.title('Number of Publications by Year', fontsize=16)
plt.xlabel('Publication Year', fontsize=16)
plt.ylabel('Number of Publications', fontsize=16)
plt.xticks(fontsize=16)
plt.yticks(fontsize=16)
plt.grid(True, axis='y')
plt.show()

import pandas as pd
import matplotlib.pyplot as plt

# Load the data from the provided Excel file

# Extract relevant columns and clean the data
df = df[['Publication Year', 'Abstract']].dropna()

# Count the number of publications by year
publications_per_year = df.groupby('Publication Year').size()

# Calculate the cumulative growth of publications over time
cumulative_growth = publications_per_year.cumsum()

# Plot the cumulative growth
plt.figure(figsize=(10, 6))
plt.plot(cumulative_growth.index, cumulative_growth.values, marker='o')
plt.title('Cumulative Growth in Research Publications Over Time')
plt.xlabel('Year')
plt.ylabel('Cumulative Number of Publications')
plt.grid(True)
plt.show()

import pandas as pd

# Load the Excel file
file_path = '/content/drive/MyDrive/savedrecs_tiktok.xlsx'
excel_data = pd.ExcelFile(file_path)
df = excel_data.parse('savedrecs')
df.columns

# Pair Plot for numerical columns (e.g., citations)
numeric_columns = df[['Times Cited, All Databases', 'Times Cited, WoS Core', 'Cited Reference Count']]
sns.pairplot(numeric_columns)
plt.suptitle('Pair Plot of Citations and References', y=1.02, fontsize=16)
plt.show()

# Heatmap for Correlation between numerical columns
correlation_matrix = numeric_columns.corr()
plt.figure(figsize=(10, 8))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', linewidths=0.5)
plt.title('Correlation Matrix of Citation Metrics', fontsize=16)
plt.show()

# Word Cloud for Author Keywords
from wordcloud import WordCloud

keywords_text = ' '.join(keywords_flat)
wordcloud = WordCloud(width=800, height=400, background_color='white').generate(keywords_text)

plt.figure(figsize=(12, 6))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.title('Word Cloud of Author Keywords', fontsize=16)
plt.show()

# Violin Plot for Citations per Year
plt.figure(figsize=(12, 6))
sns.violinplot(x='Publication Year', y='Times Cited, All Databases', data=df, palette='coolwarm', scale='width')
plt.title('Distribution of Citations per Year', fontsize=16)
plt.xlabel('Publication Year', fontsize=14)
plt.ylabel('Citations (All Databases)', fontsize=14)
plt.grid(True)
plt.show()

# Identify the top 10 most cited articles
top_10_cited = df.nlargest(10, 'Times Cited, All Databases')[['Article Title', 'Times Cited, All Databases']]

# Plot the top 10 most cited articles
plt.figure(figsize=(12, 6))
sns.barplot(x='Times Cited, All Databases', y='Article Title', data=top_10_cited, palette='Blues_d')
plt.title('Top 10 Most Cited Articles', fontsize=20)
plt.xlabel('Times Cited (All Databases)', fontsize=20)
plt.ylabel('Article Title', fontsize=18)
plt.xticks(fontsize=20)
plt.yticks(fontsize=20)
plt.grid(True, axis='x')
plt.show()

# Count the number of publications per research area
research_area_counts = df['Research Areas'].str.split(';').explode().str.strip().value_counts().head(10)

# Plot the number of publications per research area
plt.figure(figsize=(12, 6))
sns.barplot(x=research_area_counts.values, y=research_area_counts.index, palette='viridis')
plt.title('Number of Publications by Research Area (Top 10)', fontsize=16)
plt.xlabel('Number of Publications', fontsize=16)
plt.ylabel('Research Area', fontsize=16)
plt.xticks(fontsize=16)
plt.yticks(fontsize=16)
plt.grid(True, axis='x')
plt.show()

# Extract and count the frequency of each keyword by year
keywords_by_year = df.dropna(subset=['Author Keywords']).copy()
keywords_by_year['Author Keywords'] = keywords_by_year['Author Keywords'].str.split(';')
keywords_by_year = keywords_by_year.explode('Author Keywords')
keywords_by_year['Author Keywords'] = keywords_by_year['Author Keywords'].str.strip().str.lower()

# Filter to include only the top 5 keywords
top_5_keywords = [keyword for keyword, _ in keywords_counter.most_common(5)]
keywords_by_year = keywords_by_year[keywords_by_year['Author Keywords'].isin(top_5_keywords)]

# Group by year and keyword
keyword_trends = keywords_by_year.groupby(['Publication Year', 'Author Keywords']).size().unstack().fillna(0)

# Plot the publication trend for top 5 keywords
plt.figure(figsize=(12, 6))
keyword_trends.plot(kind='line', marker='o', linewidth=2.5, figsize=(12, 6))
plt.title('Publication Trend for Top 5 Keywords', fontsize=16)
plt.xlabel('Publication Year', fontsize=14)
plt.ylabel('Number of Publications', fontsize=14)
plt.grid(True)
plt.show()

from collections import Counter
import networkx as nx

# Extracting affiliations and cleaning the data
affiliations_series = df['Affiliations'].dropna().str.split('; ')
all_affiliations = [affiliation for sublist in affiliations_series for affiliation in sublist]
affiliation_counts = Counter(all_affiliations)

# Identifying the top 15 most frequent institutions for a more focused yet still broad analysis
top_15_institutions = [institution for institution, count in affiliation_counts.most_common(10)]

# Building a graph to represent collaborations between these top 15 institutions
G_15 = nx.Graph()

for institutions in affiliations_series:
    # Filter institutions to include only the top 15 for each publication
    filtered_institutions = [inst for inst in institutions if inst in top_15_institutions]
    for i in range(len(filtered_institutions)):
        for j in range(i + 1, len(filtered_institutions)):
            if filtered_institutions[i] != filtered_institutions[j]:
                if G_15.has_edge(filtered_institutions[i], filtered_institutions[j]):
                    G_15[filtered_institutions[i]][filtered_institutions[j]]['weight'] += 1
                else:
                    G_15.add_edge(filtered_institutions[i], filtered_institutions[j], weight=1)

# Drawing the collaboration network for the top 15 institutions
plt.figure(figsize=(12, 12))
pos_15 = nx.spring_layout(G_15, k=0.3, iterations=40)
nx.draw_networkx(G_15, pos_15, node_size=5500, node_color='lightgreen', with_labels=True, font_weight='bold', width=2, edge_color='grey')
labels_15 = nx.get_edge_attributes(G_15, 'weight')
nx.draw_networkx_edge_labels(G_15, pos_15, edge_labels=labels_15)
plt.title('Collaboration Network of Top 15 Institutions in Energy Storage Research')
plt.axis('off')
plt.show()

# Adjusting the graph for the top 15 institutions to ensure the entire names of the institutions are visible and not truncated
plt.figure(figsize=(15, 15))
pos_15_adjusted = nx.spring_layout(G_15, k=0.35, iterations=40)
nx.draw_networkx(G_15, pos_15_adjusted, node_size=5500, node_color='lightgreen', with_labels=True, font_size=12, font_weight='bold', width=2, edge_color='grey')
labels_15_adjusted = nx.get_edge_attributes(G_15, 'weight')
nx.draw_networkx_edge_labels(G_15, pos_15_adjusted, edge_labels=labels_15_adjusted, font_size=11)
plt.axis('off')
plt.show()

from itertools import combinations

# Preparing data for keyword co-occurrence analysis
# Limiting the analysis to the top 20 keywords for clarity
author_keywords_series = df['Author Keywords'].dropna().str.split('; ').explode()
author_keywords_counts = author_keywords_series.value_counts().head(10)  # Top 10 most frequent keywords

author_keywords_counts

# Combining similar keywords (case-insensitive, singular/plural) for a more accurate count
combined_keywords = author_keywords_series.str.lower().str.replace('storage', 'storage').value_counts().head(10)

top_20_keywords = combined_keywords.head(10).index.tolist()

# Creating a dataframe for co-occurrence calculation
co_occurrence_data = df[df['Author Keywords'].notnull()]['Author Keywords'].str.lower()

# Initializing a dictionary to count co-occurrences
co_occurrence_counts = {pair: 0 for pair in combinations(top_20_keywords, 2)}

# Counting co-occurrences
for keywords_list in co_occurrence_data.str.split('; '):
    filtered_keywords = [keyword for keyword in keywords_list if keyword in top_20_keywords]
    for pair in combinations(filtered_keywords, 2):
        if pair in co_occurrence_counts:
            co_occurrence_counts[pair] += 1
        elif (pair[1], pair[0]) in co_occurrence_counts:
            co_occurrence_counts[(pair[1], pair[0])] += 1

# Building the co-occurrence network
G_co_occurrence = nx.Graph()

# Adding nodes
for keyword in top_20_keywords:
    G_co_occurrence.add_node(keyword)

# Adding edges based on co-occurrence counts
for pair, weight in co_occurrence_counts.items():
    if weight > 0:  # Adding only if there's at least one co-occurrence
        G_co_occurrence.add_edge(pair[0], pair[1], weight=weight)

# Drawing the co-occurrence network
plt.figure(figsize=(16, 16))
pos_co_occurrence = nx.spring_layout(G_co_occurrence, k=0.15, iterations=50)
nx.draw_networkx(G_co_occurrence, pos_co_occurrence, node_size=4000, node_color='orange', with_labels=True, font_size=14, font_weight='bold', width=2, edge_color='pink')
labels_co_occurrence = nx.get_edge_attributes(G_co_occurrence, 'weight')
nx.draw_networkx_edge_labels(G_co_occurrence, pos_co_occurrence, edge_labels=labels_co_occurrence, font_color='red')
plt.title('Keyword Co-occurrence Map in Prosumer Research', fontsize=14)
plt.axis('off')
plt.show()

# Preparing data for Research Area and Keyword Relationships
# Using top 10 research areas and top 20 keywords for this analysis
research_areas_series = df['Research Areas'].str.split('; ').explode()
research_areas_counts = research_areas_series.value_counts().head(10)  # Top 15 for broader analysis
top_10_research_areas = research_areas_counts.head(10).index.tolist()

# Creating a dictionary to count occurrences of keywords within each research area
area_keyword_counts = {(area, keyword): 0 for area in top_10_research_areas for keyword in top_20_keywords}

# Counting occurrences
for _, row in df.iterrows():
    if pd.isna(row['Author Keywords']) or pd.isna(row['Research Areas']):
        continue
    keywords = row['Author Keywords'].lower().split('; ')
    areas = row['Research Areas'].split('; ')
    for area in areas:
        if area in top_10_research_areas:
            for keyword in keywords:
                if keyword in top_20_keywords:
                    area_keyword_counts[(area, keyword)] += 1

# Building the network
G_area_keyword = nx.Graph()

# Adding nodes for both areas and keywords
for area in top_10_research_areas:
    G_area_keyword.add_node(area, type='area', color='lightblue')
for keyword in top_20_keywords:
    G_area_keyword.add_node(keyword, type='keyword', color='lightgreen')

# Adding edges based on occurrences of keywords within research areas
for (area, keyword), weight in area_keyword_counts.items():
    if weight > 0:  # Adding only if the keyword appears at least once in the area
        G_area_keyword.add_edge(area, keyword, weight=weight)

# Drawing the network
plt.figure(figsize=(16, 16))
colors = nx.get_node_attributes(G_area_keyword, 'color').values()
pos_area_keyword = nx.spring_layout(G_area_keyword, k=0.3, iterations=50)
nx.draw_networkx(G_area_keyword, pos_area_keyword, node_size=4000, node_color=colors, with_labels=True, font_size=14, font_weight='bold', width=2, edge_color='pink')
plt.axis('off')
plt.show()

import plotly.graph_objects as go

# Assuming you have a DataFrame called 'keywords_exploded'
# Filter the keywords related to the market

keywords_data = df[['Author Keywords', 'Publication Year', 'Research Areas']]

# Split the keywords into individual entries
keywords_data['Keywords'] = keywords_data['Author Keywords'].str.split(';')

import pandas as pd
from nltk.stem import WordNetLemmatizer
import nltk

# Make sure to download the WordNet data for lemmatization
nltk.download('wordnet')

# Initialize the lemmatizer
lemmatizer = WordNetLemmatizer()

synonym_mapping = {
    'blockchain': 'Blockchain', 'internet': 'Internet', 'AI': 'artificial intelligence', 'Artificial intelligence':'artificial intelligence'
    # Add more mappings if necessary
}

# Function to clean and standardize a list of keywords
def clean_keyword_list(keyword_list):
    if isinstance(keyword_list, list):
        cleaned_list = []
        for keyword in keyword_list:
            # Convert to lowercase and lemmatize each word in the keyword
            cleaned_keyword = ' '.join([lemmatizer.lemmatize(word) for word in keyword.lower().split()])
            # Replace synonyms with the standard term
            for phrase, standard in synonym_mapping.items():
                cleaned_keyword = cleaned_keyword.replace(phrase, standard)
            cleaned_list.append(cleaned_keyword)
        return cleaned_list
    else:
        return keyword_list  # If it's not a list, return as is

# Apply the cleaning function to each list in the Keywords column
keywords_data['Keywords'] = keywords_data['Keywords'].apply(clean_keyword_list)

# Explode the keywords into individual rows
keywords_exploded = keywords_data.explode('Keywords')

sankey_data = keywords_exploded.copy()

market_keywords_data = sankey_data[sankey_data['Keywords'].str.contains('tiktok', case=False, na=False)]


# Split the 'Research Areas' into individual areas if they are concatenated
market_keywords_data['Research Areas'] = market_keywords_data['Research Areas'].str.split(';')

# Explode the research areas so that each area is treated as a separate row
market_keywords_data = market_keywords_data.explode('Research Areas')

# Clean up research areas by stripping extra spaces
market_keywords_data['Research Areas'] = market_keywords_data['Research Areas'].str.strip()

# Filter for years between 2018 and 2024
market_keywords_data = market_keywords_data[market_keywords_data['Publication Year'].between(2018, 2024)]

# Group by keywords, publication year, and individual research areas, summing up the occurrences
market_keywords_grouped = market_keywords_data.groupby(['Keywords', 'Publication Year', 'Research Areas']).size().reset_index(name='Count')

# Identify the top 10 market-related keywords
top_10_market_keywords = market_keywords_grouped.groupby('Keywords')['Count'].sum().sort_values(ascending=False).head(10).index

# Identify the top 10 research areas based on frequency in the filtered data
top_research_areas = market_keywords_grouped.groupby('Research Areas')['Count'].sum().sort_values(ascending=False).head(10).index

# Filter the data to include only these top keywords and research areas
simplified_market_data = market_keywords_grouped[
    (market_keywords_grouped['Keywords'].isin(top_10_market_keywords)) &
    (market_keywords_grouped['Research Areas'].isin(top_research_areas))
]

# Create lists for the Sankey diagram
keywords = simplified_market_data['Keywords'].unique()
research_areas = list(top_research_areas)  # Convert to list to maintain order
years = sorted(simplified_market_data['Publication Year'].unique())

# Create labels for the nodes
#labels = list(keywords) + list(map(str, years))+ research_areas
labels = list(keywords) + research_areas + list(map(str, years))

# Define sources and targets for the connections
source = []
target = []
value = []

for _, row in simplified_market_data.iterrows():
    keyword_idx = labels.index(row['Keywords'])
    year_idx = labels.index(str(row['Publication Year']))
    research_area_idx = labels.index(row['Research Areas'])



    #year_idx = labels.index(str(row['Publication Year'])) + len(keywords)
    #research_area_idx = labels.index(row['Research Areas']) + len(keywords) + len(years)

    """# Link keyword to research area
    source.append(keyword_idx)
    target.append(research_area_idx)
    value.append(row['Count'])

    # Link research area to publication year
    source.append(research_area_idx)
    target.append(year_idx)
    value.append(row['Count'])"""

    # Link keyword to research area
    source.append(keyword_idx)
    target.append(year_idx)
    value.append(row['Count'])

    # Link research area to publication year
    source.append(year_idx)
    target.append(research_area_idx)
    value.append(row['Count'])

import matplotlib.pyplot as plt
import matplotlib.colors as mcolors
import plotly.graph_objects as go


# Shorten labels if they are too long
#labels = [label if len(label) <= 20 else label[:5] + '...' for label in labels]
#print(labels)

# Increase padding between nodes to avoid overlap
node_pad = 70  # Increase this value to create more space between nodes

# Create the Sankey diagram
fig = go.Figure(go.Sankey(
    node = dict(
      pad = 10,
      thickness = 15,
      line = dict(color = "gray", width = 0.5),
      label = labels,
      color = ["#4C72B0", "#DD8452", "#55A868", "#C44E52", "#8172B2", "#937860", "#DA8BC3", "#8C8C8C", "#CCB974", "#64B5CD"] * 5
    ),
    link = dict(
      source = source,
      target = target,
      value = value,
      color = "rgba(0,0,0,0.2)"
  )))


fig.update_layout(
    title_text="Sankey Diagram Linking Top TikTok-Related Keywords, Research Areas, and Publication Years",
    font_size=16,
    height=800,  # Further increase the height to provide more vertical space
    width=1200  # Further increase the width for better clarity
)
fig.show()

import pandas as pd


# Load the Excel file
new_file_path = '/content/drive/MyDrive/savedrecs_tiktok.xlsx'
excel_data = pd.ExcelFile(new_file_path)
new_data = excel_data.parse('savedrecs')

# Define a basic sentiment analysis function
def basic_sentiment_analysis(text):
    positive_words = ['good', 'great', 'positive', 'successful', 'excellent', 'improve', 'beneficial', 'better', 'enhance', 'progress']
    negative_words = ['data privacy', 'concern', 'cybersecurity', 'security', 'bad', 'poor', 'negative', 'fail', 'unsuccessful', 'problem', 'worse', 'issue', 'addiction', 'mental health', 'depression', 'suicide', 'misuse','fake', 'misinformation', 'inference', 'discorder']

    text = text.lower()
    positive_count = sum(word in text for word in positive_words)
    negative_count = sum(word in text for word in negative_words)
    return {'positive': positive_count, 'negative': negative_count, 'neutral': len(text.split()) - (positive_count + negative_count)}

# Get the unique publication years
years = new_data['Publication Year'].unique()

# Initialize a dictionary to hold the sentiment results by year
sentiment_by_year_dict = {}

for year in years:
    abstracts_for_year = new_data[new_data['Publication Year'] == year]['Abstract'].dropna().astype(str)

    # Apply the basic sentiment analysis to each abstract and store the results
    sentiments = [basic_sentiment_analysis(abstract) for abstract in abstracts_for_year]

    # Convert the list of dictionaries to a DataFrame
    sentiments_df = pd.DataFrame(sentiments)

    # Calculate average sentiment for the year
    avg_sentiment = sentiments_df.mean()

    # Store the results
    sentiment_by_year_dict[year] = avg_sentiment

# Convert the results to a DataFrame for better visualization
sentiment_by_year_df = pd.DataFrame(sentiment_by_year_dict).T

# Display the sentiment analysis results by year
sentiment_by_year_df

import pandas as pd
from nltk.sentiment import SentimentIntensityAnalyzer
import nltk

# Download the VADER lexicon
nltk.download('vader_lexicon')

# Initialize the sentiment intensity analyzer
sia = SentimentIntensityAnalyzer()

# Function to analyze sentiment using VADER
def vader_sentiment_analysis(text):
    sentiment = sia.polarity_scores(text)
    return sentiment

# Apply VADER sentiment analysis to the Abstract column by year
years = new_data['Publication Year'].unique()
sentiment_by_year_dict = {}

for year in years:
    abstracts_for_year = new_data[new_data['Publication Year'] == year]['Abstract'].dropna().astype(str)

    # Apply VADER to each abstract and calculate the average sentiment for the year
    sentiments = [vader_sentiment_analysis(abstract) for abstract in abstracts_for_year]
    sentiments_df = pd.DataFrame(sentiments)
    avg_sentiment = sentiments_df.mean()

    # Store the average sentiment scores for the year
    sentiment_by_year_dict[year] = avg_sentiment

# Convert the results to a DataFrame for better visualization
sentiment_by_year_df = pd.DataFrame(sentiment_by_year_dict).T

# Display the sentiment analysis results by year
sentiment_by_year_df

import pandas as pd
from textblob import TextBlob

# Function to analyze sentiment using TextBlob
def textblob_sentiment_analysis(text):
    blob = TextBlob(text)
    sentiment = {
        'polarity': blob.sentiment.polarity,    # Polarity score ranges from -1 (negative) to 1 (positive)
        'subjectivity': blob.sentiment.subjectivity  # Subjectivity score ranges from 0 (objective) to 1 (subjective)
    }
    return sentiment

new_file_path = '/content/drive/MyDrive/savedrecs_tiktok.xlsx'
excel_data = pd.ExcelFile(new_file_path)
new_data = excel_data.parse('savedrecs')

# Get the unique publication years
years = new_data['Publication Year'].unique()

# Initialize a dictionary to hold the sentiment results by year
sentiment_by_year_dict = {}

for year in years:
    abstracts_for_year = new_data[new_data['Publication Year'] == year]['Abstract'].dropna().astype(str)

    # Apply TextBlob to each abstract and calculate the average sentiment for the year
    sentiments = [textblob_sentiment_analysis(abstract) for abstract in abstracts_for_year]
    sentiments_df = pd.DataFrame(sentiments)
    avg_sentiment = sentiments_df.mean()

    # Store the average sentiment scores for the year
    sentiment_by_year_dict[year] = avg_sentiment

# Convert the results to a DataFrame for better visualization
sentiment_by_year_df = pd.DataFrame(sentiment_by_year_dict).T

# Display the sentiment analysis results by year
sentiment_by_year_df

import re

# Function to search for "market" plus/minus one word before or after in the text
def search_market_context(text):
    # Regular expression to match "market" with one word before or after
    pattern = r'\b\w+\stiktok\b|\btiktok\s\w+\b'
    matches = re.findall(pattern, text.lower())
    return len(matches)

# Apply the function to the Abstract column
market_context_counts = new_data['Abstract'].dropna().astype(str).apply(search_market_context)

# Sum up the total count of appearances across all abstracts
total_market_context_count = market_context_counts.sum()

total_market_context_count

"""The term "market," along with one word before or after it, appears 12,805 times across all the abstracts in your dataset. This count includes instances where "market" is used in various contexts, reflecting its frequent mention in the research abstracts."""

import plotly.graph_objects as go

# Assuming you have a DataFrame called 'keywords_exploded'
# Filter the keywords related to the market

keywords_data = new_data[['Author Keywords', 'Publication Year', 'Research Areas']]

# Split the keywords into individual entries
keywords_data['Keywords'] = keywords_data['Author Keywords'].str.split(';')

import pandas as pd
from nltk.stem import WordNetLemmatizer
import nltk

# Make sure to download the WordNet data for lemmatization
nltk.download('wordnet')

# Initialize the lemmatizer
lemmatizer = WordNetLemmatizer()

synonym_mapping = {
    'electricity market': 'energy market', 'market power': 'power market',
    # Add more mappings if necessary
}

# Function to clean and standardize a list of keywords
def clean_keyword_list(keyword_list):
    if isinstance(keyword_list, list):
        cleaned_list = []
        for keyword in keyword_list:
            # Convert to lowercase and lemmatize each word in the keyword
            cleaned_keyword = ' '.join([lemmatizer.lemmatize(word) for word in keyword.lower().split()])
            # Replace synonyms with the standard term
            for phrase, standard in synonym_mapping.items():
                cleaned_keyword = cleaned_keyword.replace(phrase, standard)
            cleaned_list.append(cleaned_keyword)
        return cleaned_list
    else:
        return keyword_list  # If it's not a list, return as is

# Apply the cleaning function to each list in the Keywords column
keywords_data['Keywords'] = keywords_data['Keywords'].apply(clean_keyword_list)

# Explode the keywords into individual rows
keywords_exploded = keywords_data.explode('Keywords')

sankey_data = keywords_exploded.copy()

market_keywords_data = sankey_data[sankey_data['Keywords'].str.contains('tiktok', case=False, na=False)]


# Split the 'Research Areas' into individual areas if they are concatenated
market_keywords_data['Research Areas'] = market_keywords_data['Research Areas'].str.split(';')

# Explode the research areas so that each area is treated as a separate row
market_keywords_data = market_keywords_data.explode('Research Areas')

# Clean up research areas by stripping extra spaces
market_keywords_data['Research Areas'] = market_keywords_data['Research Areas'].str.strip()

# Filter for years between 2018 and 2024
market_keywords_data = market_keywords_data[market_keywords_data['Publication Year'].between(2018, 2024)]

# Group by keywords, publication year, and individual research areas, summing up the occurrences
market_keywords_grouped = market_keywords_data.groupby(['Keywords', 'Publication Year', 'Research Areas']).size().reset_index(name='Count')

# Identify the top 10 market-related keywords
top_10_market_keywords = market_keywords_grouped.groupby('Keywords')['Count'].sum().sort_values(ascending=False).head(10).index

# Identify the top 10 research areas based on frequency in the filtered data
top_research_areas = market_keywords_grouped.groupby('Research Areas')['Count'].sum().sort_values(ascending=False).head(10).index

# Filter the data to include only these top keywords and research areas
simplified_market_data = market_keywords_grouped[
    (market_keywords_grouped['Keywords'].isin(top_10_market_keywords)) &
    (market_keywords_grouped['Research Areas'].isin(top_research_areas))
]

# Create lists for the Sankey diagram
keywords = simplified_market_data['Keywords'].unique()
research_areas = list(top_research_areas)  # Convert to list to maintain order
years = sorted(simplified_market_data['Publication Year'].unique())

# Create labels for the nodes
#labels = list(keywords) + list(map(str, years))+ research_areas
labels = list(keywords) + research_areas + list(map(str, years))

# Define sources and targets for the connections
source = []
target = []
value = []

for _, row in simplified_market_data.iterrows():
    keyword_idx = labels.index(row['Keywords'])
    year_idx = labels.index(str(row['Publication Year']))
    research_area_idx = labels.index(row['Research Areas'])



    #year_idx = labels.index(str(row['Publication Year'])) + len(keywords)
    #research_area_idx = labels.index(row['Research Areas']) + len(keywords) + len(years)

    """# Link keyword to research area
    source.append(keyword_idx)
    target.append(research_area_idx)
    value.append(row['Count'])

    # Link research area to publication year
    source.append(research_area_idx)
    target.append(year_idx)
    value.append(row['Count'])"""

    # Link keyword to research area
    source.append(keyword_idx)
    target.append(year_idx)
    value.append(row['Count'])

    # Link research area to publication year
    source.append(year_idx)
    target.append(research_area_idx)
    value.append(row['Count'])

import matplotlib.pyplot as plt
import matplotlib.colors as mcolors
import plotly.graph_objects as go


# Shorten labels if they are too long
#labels = [label if len(label) <= 20 else label[:5] + '...' for label in labels]
#print(labels)

# Increase padding between nodes to avoid overlap
node_pad = 70  # Increase this value to create more space between nodes

# Create the Sankey diagram
fig = go.Figure(go.Sankey(
    node = dict(
      pad = 10,
      thickness = 15,
      line = dict(color = "gray", width = 0.5),
      label = labels,
      color = ["#4C72B0", "#DD8452", "#55A868", "#C44E52", "#8172B2", "#937860", "#DA8BC3", "#8C8C8C", "#CCB974", "#64B5CD"] * 5
    ),
    link = dict(
      source = source,
      target = target,
      value = value,
      color = "rgba(0,0,0,0.2)"
  )))


fig.update_layout(
    title_text="Sankey Diagram Linking Top TikTok-Related Keywords, Research Areas, and Publication Years",
    font_size=14,
    height=800,  # Further increase the height to provide more vertical space
    width=1200  # Further increase the width for better clarity
)
fig.show()

# Analyzing the keywords and their yearly distribution

# Extract the relevant columns for analysis
keywords_data = new_data[['Author Keywords', 'Publication Year']]

# Split the keywords into individual entries
keywords_data['Keywords'] = keywords_data['Author Keywords'].str.split(';')

# Explode the keywords into individual rows
keywords_exploded = keywords_data.explode('Keywords')

# Clean up any leading/trailing whitespace in the keywords
keywords_exploded['Keywords'] = keywords_exploded['Keywords'].str.strip()

# Group by keywords and publication year, then count occurrences
keywords_yearly_distribution = keywords_exploded.groupby(['Keywords', 'Publication Year']).size().unstack(fill_value=0)

# Summarize the keyword distribution over the years
keyword_summary = keywords_yearly_distribution.sum(axis=1).sort_values(ascending=False)

# Show the top 10 most frequent keywords
top_keywords = keyword_summary.head(30)

# Display the yearly distribution of the top 10 keywords
top_keywords_distribution = keywords_yearly_distribution.loc[top_keywords.index]

import matplotlib.pyplot as plt

# Plot the yearly distribution of the top keywords
plt.figure(figsize=(14, 8))

# Iterate through the top keywords and plot their distribution over the years
for keyword in top_keywords_distribution.index:
    plt.plot(top_keywords_distribution.columns, top_keywords_distribution.loc[keyword], marker='o', label=keyword)

# Adding labels and title
plt.title('Yearly Distribution of Top SSG Keywords (2019-2024)', fontsize=16)
plt.xlabel('Year', fontsize=14)
plt.ylabel('Number of Mentions', fontsize=14)
plt.xticks(top_keywords_distribution.columns)
plt.legend(title='Keywords', fontsize=12)
plt.grid(True)

# Display the plot
plt.show()

# Define a list of specific market types to search for, taking into account different cases and plural/singular forms
market_types_specific = [
    'political', 'politic', 'politics', 'election',
    'vote', 'voting', 'influence', 'impact', 'sovereignty', 'interference', 'intervention'
]

# Function to search for the specific market types in the text
def search_specific_markets(text):
    text = text.lower()  # Convert text to lowercase to handle case insensitivity
    count = 0
    for market in market_types_specific:
        count += len(re.findall(r'\b' + re.escape(market) + r'\b', text))
    return count

# Apply the function to the Abstract column
specific_market_counts = new_data['Abstract'].dropna().astype(str).apply(search_specific_markets)

# Sum up the total count of appearances across all abstracts
total_specific_market_count = specific_market_counts.sum()

total_specific_market_count

# Function to search for each specific market type separately and count their appearances
def count_market_types(text):
    text = text.lower()  # Convert text to lowercase to handle case insensitivity
    counts = {market: len(re.findall(r'\b' + re.escape(market) + r'\b', text)) for market in market_types_specific}
    return counts

# Apply the function to the Abstract column
market_type_counts = new_data['Abstract'].dropna().astype(str).apply(count_market_types)

# Sum up the counts for each market type across all abstracts
total_market_type_counts = pd.DataFrame(list(market_type_counts)).sum()

total_market_type_counts

