# -*- coding: utf-8 -*-
"""BERT-Clustering_tik.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1dl-IgLXZn1cTjZ8A07EpbXRWxN3s5okb
"""

!pip install umap sentence-transformers umap-learn hdbscan sklearn pandas

!pip install hdbscan

!pip install umap-learn

import pandas as pd
from sentence_transformers import SentenceTransformer
from sklearn.decomposition import PCA
import umap
import hdbscan
from sklearn.feature_extraction.text import CountVectorizer
from collections import Counter

# Load dataset
file_path = '/content/drive/MyDrive/savedrecs_tiktok.xlsx'  # Update to your file path
df = pd.read_excel(file_path)

# Ensure the "Abstract" column exists
if 'Abstract' not in df.columns:
    raise ValueError("The dataset does not contain an 'Abstract' column.")

# Drop rows with NaN in "Abstract" and reset the index
df = df.dropna(subset=['Abstract']).reset_index(drop=True)

abstracts = df['Abstract'].tolist()

# Step 1: Generate embeddings using BERT
model = SentenceTransformer('all-MiniLM-L6-v2')  # Lightweight BERT model
embeddings = model.encode(abstracts, show_progress_bar=True)

# Step 2: Dimensionality reduction using UMAP
umap_model = umap.UMAP(n_neighbors=15, n_components=5, random_state=42)
reduced_embeddings = umap_model.fit_transform(embeddings)

# Step 3: Clustering using HDBSCAN
cluster_model = hdbscan.HDBSCAN(min_cluster_size=20, min_samples=10, metric='euclidean', cluster_selection_epsilon=0.5,  cluster_selection_method='eom')
clusters = cluster_model.fit_predict(reduced_embeddings)

# Add cluster labels to the dataset
df['Cluster'] = clusters

# Step 4: Extract keywords for each cluster
def extract_keywords(texts, cluster_labels, cluster_id):
    cluster_texts = [texts[i] for i in range(len(texts)) if cluster_labels[i] == cluster_id]
    vectorizer = CountVectorizer(stop_words='english', max_features=20)
    bow_matrix = vectorizer.fit_transform(cluster_texts)
    word_counts = bow_matrix.sum(axis=0).A1
    keywords = [vectorizer.get_feature_names_out()[i] for i in word_counts.argsort()[::-1]]
    return keywords

# Generate keywords for each cluster
topics = {}
for cluster_id in set(clusters):
    if cluster_id == -1:  # Skip noise points
        continue
    keywords = extract_keywords(abstracts, clusters, cluster_id)
    topics[cluster_id] = keywords

# Step 5: Display topics
for cluster_id, keywords in topics.items():
    print(f"Cluster {cluster_id}: {', '.join(keywords)}")

import matplotlib.pyplot as plt
plt.scatter(reduced_embeddings[:, 0], reduced_embeddings[:, 1], c=clusters, cmap='Spectral')
plt.colorbar()
plt.title("UMAP Clusters")
plt.show()

"""Based on the keywords associated with each cluster, here are the suggested names that characterize their main themes:

1. **Cluster 0: TikTok and General Health Content**  
   Focuses on TikTok’s role in disseminating health-related information, content quality, and platform usage for health communication.

2. **Cluster 1: TikTok's Global Reach and Chinese Influence**  
   Highlights TikTok’s international impact, its Chinese origins, and comparisons with its Chinese version, Douyin.

3. **Cluster 2: TikTok in Education and Learning**  
   Explores TikTok’s applications in educational contexts, including language learning, teaching strategies, and online education.

4. **Cluster 3: TikTok’s Sociopolitical and Digital Influence**  
   Captures TikTok’s role in social and political contexts, including its impact on platforms, political communication, and user engagement.

These names summarize the thematic focus of each cluster while maintaining clarity and relevance to the keywords provided. Let me know if you'd like further refinement!
"""

plt.scatter(reduced_embeddings[:, 0], reduced_embeddings[:, 1], c=clusters, cmap='tab10', s=10)
plt.colorbar(label='Cluster ID')

valid_points = clusters != -1
plt.scatter(reduced_embeddings[valid_points, 0], reduced_embeddings[valid_points, 1], c=clusters[valid_points], cmap='tab10', s=20)